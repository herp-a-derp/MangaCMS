

import settings
import os
import os.path



import urllib.parse
import html.parser
import zipfile
import runStatus
import traceback
import bs4
import re

import nameTools as nt

import MangaCMS.cleaner.processDownload
import MangaCMS.ScrapePlugins.RetreivalBase

class ContentLoader(MangaCMS.ScrapePlugins.RetreivalBase.RetreivalBase):



	logger_path  = "Main.Manga.Mc.Cl"
	plugin_name  = "MangaCow Content Retreiver"
	plugin_key   = "mc"
	is_manga    = True
	is_hentai   = False
	is_book     = False

	retreivalThreads = 2


	def getImage(self, imageUrl, referrer):

		content, handle = self.wg.getpage(imageUrl, returnMultiple=True, addlHeaders={'Referer': referrer})
		if not content or not handle:
			raise ValueError("Failed to retreive image from page '%s'!" % referrer)

		fileN = urllib.parse.unquote(urllib.parse.urlparse(handle.geturl())[2].split("/")[-1])
		fileN = bs4.UnicodeDammit(fileN).unicode_markup
		self.log.info("retreived image '%s' with a size of %0.3f K", fileN, len(content)/1000.0)
		return fileN, content



	def getImageUrls(self, baseUrl):
		soup = self.wg.getSoup(baseUrl)

		selector = soup.find("select", class_="cbo_wpm_pag")

		if not selector:
			raise ValueError("Unable to find image selector for page '%s'" % baseUrl)

		pageNumbers = []
		for value in selector.find_all("option"):
			pageNumbers.append(int(value.get_text()))


		if not pageNumbers:
			raise ValueError("Unable to find contained images on page '%s'" % baseUrl)



		pageUrls = []
		for pageNo in pageNumbers:
			pageUrls.append("{baseUrl}{num}/".format(baseUrl=baseUrl, num=pageNo))

		# print("PageUrls", pageUrls)
		imageUrls = []

		for pageUrl in pageUrls:

			pageCtnt = self.wg.getpage(pageUrl)

			soup = bs4.BeautifulSoup(pageCtnt, "lxml")

			imageContainer = soup.find("div", class_="prw")
			url = imageContainer.img["src"]
			# print("Urls - ", (url, pageUrl))
			imageUrls.append((url, pageUrl))


		return imageUrls


	def get_link(self, link_row_id):

		with self.row_context(dbid=link_row_id) as row:
			series_name = row.series_name
			chapter_name = row.origin_name
			source_url = row.source_id
			row.state = 'fetching'


		try:
			self.log.info( "Should retreive url - %s", source_url)

			imageUrls = self.getImageUrls(source_url)

			if not imageUrls:
				with self.row_context(dbid=link_row_id) as row:
					row.state = 'error'
					row.err_str = "error-404"
					return


			images = []
			for imgUrl, referrerUrl in imageUrls:
				imageName, imageContent = self.getImage(imgUrl, referrerUrl)
				images.append([imageName, imageContent])


			if not images:
				with self.row_context(dbid=link_row_id) as row:
					row.state = 'error'
					row.err_str = "error-404"
					return


			self.save_manga_image_set(link_row_id, series_name, chapter_name, images, source_name="MangaCow")


		except Exception:
			self.log.critical("Failure on retrieving content at %s", source_url)
			self.log.critical("Traceback = %s", traceback.format_exc())
			with self.row_context(dbid=link_row_id) as row:
				row.state = 'error'
				row.err_str = traceback.format_exc()
			raise



	# def getLink(self, link):
	# 	sourceUrl  = link["sourceUrl"]
	# 	seriesName = link["seriesName"]
	# 	chapterVol = link["originName"]


	# 	try:
	# 		self.log.info( "Should retreive url - %s", sourceUrl)
	# 		self.updateDbEntry(sourceUrl, dlState=1)

	# 		imageUrls = self.getImageUrls(sourceUrl)
	# 		if not imageUrls:
	# 			self.log.critical("Failure on retrieving content at %s", sourceUrl)
	# 			self.log.critical("Page not found - 404")
	# 			self.updateDbEntry(sourceUrl, dlState=-1)
	# 			return

	# 		self.log.info("Downloading = '%s', '%s'", seriesName, chapterVol)
	# 		dlPath, newDir = self.locateOrCreateDirectoryForSeries(seriesName)

	# 		if link["flags"] == None:
	# 			link["flags"] = ""

	# 		if newDir:
	# 			self.updateDbEntry(sourceUrl, flags=" ".join([link["flags"], "haddir"]))

	# 		chapterName = nt.makeFilenameSafe(chapterVol)

	# 		fqFName = os.path.join(dlPath, chapterName+"[MangaCow].zip")

	# 		loop = 1
	# 		while os.path.exists(fqFName):
	# 			fqFName, ext = os.path.splitext(fqFName)
	# 			fqFName = "%s (%d)%s" % (fqFName, loop,  ext)
	# 			loop += 1
	# 		self.log.info("Saving to archive = %s", fqFName)

	# 		images = []
	# 		for imgUrl, referrerUrl in imageUrls:
	# 			imageName, imageContent = self.getImage(imgUrl, referrerUrl)

	# 			images.append([imageName, imageContent])

	# 			if not runStatus.run:
	# 				self.log.info( "Breaking due to exit flag being set")
	# 				self.updateDbEntry(sourceUrl, dlState=0)
	# 				return

	# 		self.log.info("Creating archive with %s images", len(images))

	# 		if not images:
	# 			self.updateDbEntry(sourceUrl, dlState=-1, seriesName=seriesName, originName=chapterVol, tags="error-404")
	# 			return

	# 		#Write all downloaded files to the archive.
	# 		arch = zipfile.ZipFile(fqFName, "w")
	# 		for imageName, imageContent in images:
	# 			arch.writestr(imageName, imageContent)
	# 		arch.close()


	# 		dedupState = MangaCMS.cleaner.processDownload.processDownload(seriesName, fqFName, deleteDups=True, includePHash=True, phashThresh=6, rowId=link['dbId'])
	# 		self.log.info( "Done")

	# 		filePath, fileName = os.path.split(fqFName)
	# 		self.updateDbEntry(sourceUrl, dlState=2, downloadPath=filePath, fileName=fileName, seriesName=seriesName, originName=chapterVol, tags=dedupState)
	# 		return



	# 	except Exception:
	# 		self.log.critical("Failure on retrieving content at %s", sourceUrl)
	# 		self.log.critical("Traceback = %s", traceback.format_exc())
	# 		self.updateDbEntry(sourceUrl, dlState=-1)



if __name__ == "__main__":
	import utilities.testBase as tb

	with tb.testSetup():
		run = ContentLoader()
		run.do_fetch_content()



